{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Operations – Basic Transformations such as filtering, aggregations, joins etc\n",
    "\n",
    "As part of this session we will see basic transformations we can perform on top of Data Frames such as filtering, aggregations, joins etc. We will build end to end application by taking a simple problem statement.\n",
    "\n",
    "* Data Frame Operations – APIs\n",
    "* Problem Statement – Get daily product revenue\n",
    "* Projecting data using select, withColumn and selectExpr\n",
    "* Filtering data using where or filter\n",
    "* Joining Data Sets\n",
    "* Grouping data and performing aggregations\n",
    "* Sorting data\n",
    "* Development Life Cycle\n",
    "\n",
    "### Data Frame Operations – APIs\n",
    "\n",
    "Let us recap about Data Frame Operations. It is one of the 2 ways we can process Data Frames.\n",
    "\n",
    "* Selection or Projection – select\n",
    "* Filtering data – filter or where\n",
    "* Joins – join (supports outer join as well)\n",
    "* Aggregations – groupBy and agg with support of functions such as sum, avg, min, max etc\n",
    "* Sorting – sort or orderBy\n",
    "* Analytics Functions – aggregations, ranking and windowing functions\n",
    "\n",
    "# Problem Statement – Get daily product revenue\n",
    "\n",
    "Here is the problem statement for which we will be exploring Data Frame APIs to come up with final solution.\n",
    "\n",
    "Get daily product revenue\n",
    "\n",
    "* orders – order_id, order_date, order_customer_id, order_status\n",
    "* order_items – order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price\n",
    "* Data is comma separated\n",
    "* We will fetch data using spark.read.csv\n",
    "* Apply type cast functions to convert fields into their original type where ever is applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sspark = SparkSession. \\\n",
    "  builder. \\\n",
    "  master('local'). \\\n",
    "  appName('CSV Example'). \\\n",
    "  getOrCreate()\n",
    "\n",
    "orders = spark.read. \\\n",
    "  format('csv'). \\\n",
    "  schema('order_id int, order_date string, order_customer_id int, order_status string'). \\\n",
    "  load('/Users/itversity/Research/data/retail_db/orders')\n",
    "\n",
    "orders.printSchema()\n",
    "orders.show()\n",
    "\n",
    "orderItems = spark.read. \\\n",
    "  format('csv'). \\\n",
    "  schema('''order_item_id int, \n",
    "            order_item_order_id int, \n",
    "            order_item_product_id int, \n",
    "            order_item_quantity int,\n",
    "            order_item_subtotal float,\n",
    "            order_item_product_price float\n",
    "         '''). \\\n",
    "  load('/Users/itversity/Research/data/retail_db/order_items')\n",
    "\n",
    "orderItems.printSchema()\n",
    "orderItems.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " In case you are using pycharm, first you need to create object of type SparkSession\n",
    "spark = SparkSession. \\\n",
    "  builder. \\\n",
    "  master('local'). \\\n",
    "  appName('CSV Example'). \\\n",
    "  getOrCreate()\n",
    "\n",
    "ordersCSV = spark.read. \\\n",
    "  csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orderItemsCSV = spark.read. \\\n",
    "  csv('/public/retail_db/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', \n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.printSchema()\n",
    "orders.show()\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))\n",
    "\n",
    "orderItems.printSchema()\n",
    "orderItems.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecting data using select, withColumn and selectExpr\n",
    "\n",
    "Now let us see how we can project data the way we want using select.\n",
    "\n",
    "* Python classes are dynamic. It means we can change the structure of the class at run time.\n",
    "* In this case both orders and orderItems are of type DataFrame, but in orders we will be able to access its attributes and in orderItems we will be able to access its attributes (e. g.: orders.order_id and orderItems.order_item_id)\n",
    "* We can use select and fetch data from the fields we are looking for.\n",
    "* We can represent data using DataFrame.ColumnName or directly ‘ColumnName’ in select clause – e.g.: <mark> orders.select(orders.order_id, orders.order_date)</mark> and <mark>orders.select('order_id', 'order_date')</mark>\n",
    "* We can apply necessary functions to manipulate data while it is being projected – <mark>orders.select(substring('order_date', 1, 7)).show()\n",
    "* We can give aliases to the derived fields using alias function – <mark> orders.select(substring('order_date', 1, 7).alias('order_month')).show()</mark>\n",
    "* If we want to add new field derived from existing fields we can use withColumn function. First argument is alias and 2nd argument is data processing logic – <mark> orders.withColumn('order_month', substring('order_date', 1, 7).alias('order_month')).show()</mark>\n",
    "\n",
    "# Filtering data using where or filter\n",
    "\n",
    "Data Frame have 2 APIs to filter the data, where and filter. They are just synonyms and you can use either of them for filtering.\n",
    "\n",
    "* You can use filter or where in 2 ways\n",
    "* One by using class.attributeName and comparing with values – e. g.: <mark>orders.where(orders.order_status == 'COMPLETE').show()</mark>\n",
    "* Other by passing conditions as literals – e. g.: <mark>orders.where('order_status = \"COMPLETE\"').show()</mark>\n",
    "* Make sure both orders and orderItems data frames are created\n",
    "* Let us see few more examples\n",
    " * Get orders which are either COMPLETE or CLOSED\n",
    " * Get orders which are either COMPLETE or CLOSED and placed in month of 2013 August\n",
    " * Get order items where order_item_subtotal is not equal to product of order_item_quantity and order_item_product_price\n",
    " * Get all the orders which are placed on first of every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get orders which are either COMPLETE or CLOSED\n",
    "orders.where('order_status = \"COMPLETE\" or order_status = \"CLOSED\"').show()\n",
    "orders.where('order_status in (\"COMPLETE\", \"CLOSED\")').show()\n",
    "orders.where((orders.order_status == 'COMPLETE') | (orders.order_status == 'CLOSED')).show()\n",
    "orders.where((orders.order_status == 'COMPLETE').__or__(orders.order_status == 'CLOSED')).show()\n",
    "orders.where(orders.order_status.isin('COMPLETE', 'CLOSED')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get orders which are either COMPLETE or CLOSED and placed in month of 2013 August\n",
    "\n",
    "orders.where('order_status in (\"COMPLETE\", \"CLOSED\") and order_date like \"2013-08%\"').show()\n",
    "orders.where(orders.order_status.isin('COMPLETE', 'CLOSED').__and__(orders.order_date.like('2013-08%'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get order items where order_item_subtotal is not equal to product of order_item_quantity and order_item_product_price\n",
    "orderItems.where('order_item_subtotal != round(order_item_quantity * order_item_product_price, 2)').show()\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "orderItems.where(orderItems.order_item_subtotal != \n",
    "                 round((orderItems.order_item_quantity * orderItems.order_item_product_price), 2)\n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the orders which are placed on first of every month\n",
    "orders.where('date_format(order_date, \"dd\") = \"01\"').show()\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "orders.where(date_format(orders.order_date, 'dd') == '01').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data Sets\n",
    "\n",
    "Quite often we need to deal with multiple data sets which are related with each other.\n",
    "\n",
    "* We need to first understand the relationship with respect to data sets\n",
    "* All our data sets have relationships defined between them.\n",
    " * orders and order_items are transaction tables. orders is parent and order_items is child. Relationship is established between the two using order_id (in order_items, it is represented as order_item_order_id)\n",
    " * We also have product catalog normalized into 3 tables – products, categories and departments (with relationships established in that order)\n",
    " * We also have customers table\n",
    " * There is relationship between customers and orders – customers is parent data set as one customer can place multiple orders.\n",
    " * There is relationship between product catalog and order_items via products – products is parent data set as one product can be ordered as part of multiple order_items.\n",
    "* Determine the type of join – inner or outer (left or right or full)\n",
    "* Data Frames have an API called join to perform joins\n",
    "* We can make the join outer by passing additional argument\n",
    "* Let us see few examples\n",
    " * Get all the order items corresponding to COMPLETE or CLOSED orders\n",
    " * Get all the orders where there are no corresponding order_items\n",
    " * Check if there are any order_items where there is no corresponding order in orders data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the order items corresponding to COMPLETE or CLOSED orders\n",
    "\n",
    "orders.where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the orders where there are no corresponding order_items\n",
    "\n",
    "orders. \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id, 'left'). \\\n",
    "  where('order_item_order_id is null'). \\\n",
    "  select('order_id', 'order_date', 'order_customer_id', 'order_status'). \\\n",
    "  show()\n",
    "\n",
    "orders. \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id, 'left'). \\\n",
    "  where(orderItems.order_item_order_id.isNull()). \\\n",
    "  select(orders.order_id, orders.order_date, orders.order_customer_id, orders.order_status). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any order_items where there is no corresponding order in orders data set\n",
    "\n",
    "orders. \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id, 'right'). \\\n",
    "  where('order_id is null'). \\\n",
    "  select('order_item_id', 'order_item_order_id'). \\\n",
    "  show()\n",
    "\n",
    "orders. \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id, 'right'). \\\n",
    "  where(orders.order_id.isNull()). \\\n",
    "  select(orderItems.order_item_id, orderItems.order_item_order_id). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping data and performing aggregations\n",
    "\n",
    "Many times we want to perform aggregations such as sum, average, minimum, maximum etc with in each group. We need to first group the data and then perform aggregation.\n",
    "\n",
    "* groupBy is the function which can be used to group the data on one or more columns\n",
    "* Once data is grouped we can perform all supported aggregations – sum, avg, min, max etc\n",
    "* We can invoke the functions directly or as part of agg\n",
    "* agg gives us more flexibility to give aliases to the derived fields\n",
    "* Let us see few examples\n",
    " * Get count by status from orders\n",
    " * Get revenue for each order id from order items\n",
    " * Get daily product revenue (order_date and order_item_product_id are part of keys, order_item_subtotal is used for aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count by status from orders\n",
    "orders.groupBy('order_status').count().show()\n",
    "orders.groupBy('order_status'). \\\n",
    "  agg(count('order_status').alias('status_count')). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get revenue for each order id from order items \n",
    "orderItems.groupBy('order_item_order_id'). \\\n",
    "  sum('order_item_subtotal'). \\\n",
    "  show()\n",
    "\n",
    "from pyspark.sql.functions import round, sum\n",
    "orderItems.groupBy('order_item_order_id'). \\\n",
    "  agg(round(sum('order_item_subtotal'), 2).alias('order_revenue')). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily product revenue \n",
    "# filter for complete and closed orders\n",
    "# groupBy order_date and order_item_product_id\n",
    "# Use agg and sum on order_item_subtotal to get revenue\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "from pyspark.sql.functions import sum, round\n",
    "orders.where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "  groupBy('order_date', 'order_item_product_id'). \\\n",
    "  agg(round(sum('order_item_subtotal'), 2).alias('revenue')). \\\n",
    "  show()\n",
    "\n",
    "orders.where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "  groupBy(orders.order_date, orderItems.order_item_product_id). \\\n",
    "  agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue')). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting data\n",
    "\n",
    "Now let us see how we can sort the data using sort or orderBy.\n",
    "\n",
    "* sort or orderBy can be used to sort the data\n",
    "* We can perform composite sorting by using multiple fields\n",
    "* By default data will be sorted in ascending order\n",
    "* We can change the order by using desc function\n",
    "* Let us see few examples\n",
    " * Sort orders by status\n",
    " * Sort orders by date and then by status\n",
    " * Sort order items by order_item_order_id and order_item_subtotal descending\n",
    " * Take daily product revenue data and sort in ascending order by date and then descending order by revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort orders by status\n",
    "orders.sort('order_status').show()\n",
    "orders.orderBy('order_status').show()\n",
    "orders.orderBy(orders.order_status).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort orders by date and then by status\n",
    "orders.sort('order_date', 'order_status').show()\n",
    "orders.orderBy(orders.order_date, orders.order_status).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort order items by order_item_order_id and order_item_subtotal descending\n",
    "orderItems. \\\n",
    "  sort(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \\\n",
    "  show()\n",
    "\n",
    "orderItems. \\\n",
    "  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take daily product revenue data and \n",
    "# sort in ascending order by date and \n",
    "# then descending order by revenue.\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "dailyProductRevenue = orders.where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "  join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "  groupBy(orders.order_date, orderItems.order_item_product_id). \\\n",
    "  agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue'))\n",
    "\n",
    "dailyProductRevenueSorted = dailyProductRevenue. \\\n",
    "  orderBy(dailyProductRevenue.order_date, dailyProductRevenue.revenue.desc())\n",
    "\n",
    "dailyProductRevenueSorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Life Cycle\n",
    "\n",
    "Let us develop the application using Pycharm and run it on the cluster.\n",
    "\n",
    "* Make sure application.properties have required input path and output path along with execution mode\n",
    "* Read orders and order_items data into data frames\n",
    "* Filter for complete and closed orders\n",
    "* Join with order_items\n",
    "* Aggregate to get revenue for each order_date and order_item_product_id\n",
    "* Sort in ascending order by date and then descending order by revenue\n",
    "* Save the output as CSV format\n",
    "* Validate using Pycharm\n",
    "* Ship it to the cluster, run it on the cluster and validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[dev]\n",
    "executionMode = local\n",
    "input.base.dir = /Users/itversity/Research/data/retail_db\n",
    "output.base.dir = /Users/itversity/Research/data/bootcamp/pyspark\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client\n",
    "input.base.dir = /public/retail_db\n",
    "output.base.dir = /user/training/bootcamp/pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser as cp, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "props = cp.RawConfigParser()\n",
    "props.read('src/main/resources/application.properties')\n",
    "env = sys.argv[1]\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName('Daily Product Revenue using Data Frame Operations').\\\n",
    "    master(props.get(env, 'executionMode')).\\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "inputBaseDir = props.get(env, 'input.base.dir')\n",
    "orders = spark.read. \\\n",
    "  format('csv'). \\\n",
    "  schema('''order_id INT,\n",
    "            order_date STRING,\n",
    "            order_customer_id INT,\n",
    "            order_status STRING\n",
    "         '''). \\\n",
    "  load(inputBaseDir + '/orders')\n",
    "\n",
    "orderItems = spark.read. \\\n",
    "  format('csv'). \\\n",
    "  schema('''order_item_id INT,\n",
    "            order_item_order_id INT,\n",
    "            order_item_product_id INT,\n",
    "            order_item_quantity INT,\n",
    "            order_item_subtotal FLOAT,\n",
    "            order_item_product_price FLOAT\n",
    "         '''). \\\n",
    "  load(inputBaseDir + '/order_items')\n",
    "\n",
    "from pyspark.sql.functions import sum, round\n",
    "dailyProductRevenue = orders. \\\n",
    "    where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "    join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "    groupBy('order_date', 'order_item_product_id'). \\\n",
    "    agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue'))\n",
    "\n",
    "dailyProductRevenueSorted = dailyProductRevenue. \\\n",
    "    orderBy(dailyProductRevenue.order_date, dailyProductRevenue.revenue.desc())\n",
    "\n",
    "outputBaseDir = props.get(env, 'output.base.dir')\n",
    "dailyProductRevenueSorted.write.csv(outputBaseDir + '/daily_product_revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  src/main/python/retail_db/df/DailyProductRevenueDFO.py \\\n",
    "  prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Try to develop programs for these exercises\n",
    "\n",
    "* Get number of closed or complete orders placed by each customer\n",
    "* Get revenue generated by each customer for the month of 2014 January (consider only closed or complete orders)\n",
    "* Get revenue generated by each product on monthly basis – get product name, month and revenue generated by each product (round off revenue to 2 decimals)\n",
    "* Get revenue generated by each product category on daily basis – get category name, date and revenue generated by each category (round off revenue to 2 decimals)\n",
    "* Get the details of the customers who never placed orders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
