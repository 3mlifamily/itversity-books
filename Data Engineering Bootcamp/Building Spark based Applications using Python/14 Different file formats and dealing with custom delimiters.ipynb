{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different file formats and dealing with custom delimiters\n",
    "\n",
    "As part of this session we will talk about dealing with different file formats and also custom delimiters in text data. We will see how to read and how to write the data. Also we will understand APIs such as persist/cache on Data Frames.\n",
    "\n",
    "* Overview of write APIs – dataframe.write\n",
    "* Overview of read APIs – spark.read\n",
    "* Supported file formats\n",
    " * csv, text (for text file formats)\n",
    " * json (using complex schema)\n",
    " * orc\n",
    " * parquet\n",
    " * avrò (3rd party)\n",
    "* Processing text data with custom delimiters\n",
    "* Persisting or Caching Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of write APIs – dataframe.write\n",
    "\n",
    "Let us see how we can write data to different targets using APIs under write on top of data frame.\n",
    "\n",
    "* Supported file formats – csv, text json, orc, parquet etc.\n",
    "* We can also write data to 3rd party supported file formats such as avro\n",
    "* Data can be written to Hive tables as well\n",
    "* We can also connect to relational databases over JDBC and save our output into remote relational databases.\n",
    "* We can also connect to any 3rd party database using relevant plugin and preserve data over there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('json'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders.write.json('/user/training/bootcampdemo/pyspark/orders_json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "table = 'retail_export.orders_export'\n",
    "\n",
    "orders.write. \\\n",
    "  format('jdbc'). \\\n",
    "  option('url', 'jdbc:mysql://ms.itversity.com'). \\\n",
    "  option('dbtable', 'retail_export.orders_export'). \\\n",
    "  option('user', 'retail_user'). \\\n",
    "  option('password', 'itversity'). \\\n",
    "  save(mode='append')\n",
    "\n",
    "orders.write. \\\n",
    "    jdbc(\"jdbc:mysql://ms.itversity.com\", table, mode='append',\n",
    "         properties={\"user\": \"retail_user\",\n",
    "                     \"password\": \"itversity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "# To create new table and insert into it\n",
    "orders.write. \\\n",
    "  format('hive'). \\\n",
    "  saveAsTable('bootcampdemo.orders_hive', mode='overwrite')\n",
    "\n",
    "orders.write.saveAsTable('bootcampdemo.orders_hive', mode='overwrite')\n",
    "\n",
    "# To insert data into existing table\n",
    "orders.write. \\\n",
    "  format('hive'). \\\n",
    "  insertInto('bootcampdemo.orders_hive', overwrite=True)\n",
    "\n",
    "orders.write.insertInto('bootcampdemo.orders_hive', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of read APIs – spark.read\n",
    "spark.read have bunch of APIs to read data from different source types.\n",
    "\n",
    "* Supported file formats- csv, text, json, orc, parquet etc\n",
    "* We can also read data from 3rd party supported file formats such as avro\n",
    "* We can read data directly from hive tables\n",
    "* JDBC – to read data from relational databases\n",
    "* There is generic API called format which can be used in conjunction with option to pass relevant arguments and then load data from either files or over JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders = spark.read. \\\n",
    "  format('json'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders = spark.read.json('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = 'retail_export.orders_export'\n",
    "\n",
    "orders = spark.read. \\\n",
    "  format('jdbc'). \\\n",
    "  option('url', 'jdbc:mysql://ms.itversity.com'). \\\n",
    "  option('dbtable', 'retail_export.orders_export'). \\\n",
    "  option('user', 'retail_user'). \\\n",
    "  option('password', 'itversity'). \\\n",
    "  load()\n",
    "\n",
    "orders = spark.read. \\\n",
    "    jdbc(\"jdbc:mysql://ms.itversity.com\", table,\n",
    "         properties={\"user\": \"retail_user\",\n",
    "                     \"password\": \"itversity\"})\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orders = spark.read. \\\n",
    "  format('hive'). \\\n",
    "  table('bootcampdemo.orders_hive')\n",
    "\n",
    "orders = spark.read.table('bootcampdemo.orders_hive')\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported file formats\n",
    "\n",
    "Let us see details about all the supported formats in Spark to create data frames and save them.\n",
    "\n",
    "* Following file formats are supported out of the box with Spark\n",
    " * text – using text (fixed length) or csv (delimited)\n",
    " * json\n",
    " * orc\n",
    " * parquet\n",
    "* Avro is available with 3rd party plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\"). \\\n",
    "  write. \\\n",
    "  format('text'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_text')\n",
    "\n",
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\"). \\\n",
    "  write. \\\n",
    "  text('/user/training/bootcampdemo/pyspark/orders_text')\n",
    "\n",
    "orders_read = spark.read.format('text'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_text')\n",
    "\n",
    "orders_read = spark.read.text('/user/training/bootcampdemo/pyspark/orders_text')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('csv'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_csv')\n",
    "\n",
    "orders.write.csv('/user/training/bootcampdemo/pyspark/orders_csv')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  format('csv'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_csv'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  csv('/user/training/bootcampdemo/pyspark/orders_csv'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('json'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders.write.json('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  format('json'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  json('/user/training/bootcampdemo/pyspark/orders_json')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('orc'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_orc')\n",
    "\n",
    "orders.write.orc('/user/training/bootcampdemo/pyspark/orders_orc')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  format('orc'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_orc')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  orc('/user/training/bootcampdemo/pyspark/orders_orc')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('parquet'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_parquet')\n",
    "\n",
    "orders.write.parquet('/user/training/bootcampdemo/pyspark/orders_parquet')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  format('parquet'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_parquet')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  parquet('/user/training/bootcampdemo/pyspark/orders_parquet')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch pyspark with avro dependencies\n",
    "# pyspark --master yarn --conf spark.ui.port=12901 --packages com.databricks:spark-avro_2.11:4.0.0\n",
    "\n",
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write. \\\n",
    "  format('com.databricks.spark.avro'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_avro')\n",
    "\n",
    "orders_read = spark.read. \\\n",
    "  format('com.databricks.spark.avro'). \\\n",
    "  load('/user/training/bootcampdemo/pyspark/orders_avro')\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text data with custom delimiters\n",
    "\n",
    "Now let us understand how to process text data with different line as well as field delimiters.\n",
    "\n",
    "* We can read text data into RDD using SparkContext’s textFile. It will treat new line character as record delimiter.\n",
    "* We have to parse each record in RDD and derive data to process further\n",
    "* With Spark Data Frames we have csv and text APIs to read text data int Data Frame\n",
    "* Both of them use new line character as record delimiter. When we use csv API to create data frame we can also specify field separator/delimiter using sep keyword argument\n",
    "* We can also specify sep while writing data into text files with any field separator or delimiter using csv API. Also we can concatenate data as part of selectExpr with delimiter of our choice and use text API.\n",
    "* Here is the example to read and write data with ascii null character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.selectExpr(\"concat(order_id, '\\00', order_date, '\\00', order_customer_id, '\\00', order_status)\"). \\\n",
    "  write. \\\n",
    "  text('/user/training/bootcampdemo/pyspark/orders_null')\n",
    "\n",
    "orders.write.csv('/user/training/bootcampdemo/pyspark/orders_null', '\\00')\n",
    "\n",
    "orders_read_csv = spark.read.csv('/user/training/bootcampdemo/pyspark/orders_null', sep='\\00'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orders_read = orders_read_csv. \\\n",
    "  withColumn('order_id', orders_read_csv.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', orders_read_csv.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At times, we might have to deal with text data where line delimiter is different than new line character.\n",
    "* In this case we need to use HDFS APIs to read data from files with custom line delimiter into RDD and process further (either using transformations/actions or data frame operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/public/yelp-dataset/yelp_review.csv\"\n",
    "\n",
    "yelpReview = sc.newAPIHadoopFile(path, \n",
    "  'org.apache.hadoop.mapreduce.lib.input.TextInputFormat', \n",
    "  'org.apache.hadoop.io.LongWritable', \n",
    "  'org.apache.hadoop.io.Text', \n",
    "  conf={'textinputformat.record.delimiter' : '\\r'})\n",
    "  \n",
    "yelpReview.count()\n",
    "\n",
    "for i in yelpReview.map(lambda r: str(r[1])).take(10): print(i)\n",
    "\n",
    "for i in yelpReview. \\\n",
    "  map(lambda r: (len(str(r[1]).split('\",\"')), 1)). \\\n",
    "  reduceByKey(lambda x, y: x + y). \\\n",
    "  collect():\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Persisting or Caching Data Frames\n",
    "Now let us see how we can persist data frames.\n",
    "\n",
    "* By default data will be streamed as data frames to executor tasks as data being processed.\n",
    "* Here is what will happen when data is read into executor task while it is being processed\n",
    " * Deserialize into object\n",
    " * Stream into memory\n",
    " * Process data by executor task by applying logic\n",
    " * Flush deserialized objects from memory as executor tasks are terminated\n",
    "* Some times we might have to read same data multiple times for processing with in the same job. By default every time data need to be deserialized and submitted to executor tasks for processing\n",
    "* To avoid deserializing into java objects when same data have to be read multiple times we can leverage caching.\n",
    "* There are 2 methods persist and cache. By default with data frames caching will be done as MEMORY_AND_DISK from Spark 2.\n",
    "* cache is shorthand method for persist at MEMORY_AND_DISK\n",
    "* This is what happens when we cache Data Frame\n",
    " * Caching will be done only when data is read at least once for processing\n",
    " * Each record will be deserialized into object\n",
    " * These deserialized objects will be cached in memory as long as they fit\n",
    " * If not, deserialized objects will be spilled out to disk\n",
    "* You can get details about different persistence levels from here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
