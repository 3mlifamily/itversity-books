{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Sqoop – Importing data to HDFS – Contd…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understood the basics of Sqoop and Sqoop import, let us, deep dive, further to understand other common features of Sqoop import.\n",
    "* Customizing Split logic\n",
    "* Auto reset to one mapper\n",
    "* File Formats and Compression\n",
    "* Filtering of Data\n",
    "* Delimiters and Handling Nulls\n",
    "* Incremental Loads\n",
    "\n",
    "### Customizing Split logic\n",
    "By default, Sqoop import uses 4 mappers and data will be divided into mutually exclusive subsets using primary key fields (as demonstrated earlier). Now let us see how to handle scenarios where we want to use non-primary key fields to split the data.\n",
    "* By default number of mappers is 4, it can be changed with <mark>--num-mappers</mark>\n",
    "* Split logic will be applied on the primary key if exists\n",
    "* If primary key does not exist and if we set number of mappers to more than 1, then sqoop import will fail\n",
    "* At that time we can use <mark>.--split-by</mark> to split on a non-key column or explicitly set <mark>--num-mappers</mark> to 1 or use <mark>--auto-reset-to-one-mapper</mark>\n",
    "* If the primary key column or the column specified in the split-by clause is a non-numeric type, then we need to use this additional argument <mark>-Dorg.apache.sqoop.splitter.allow_text_splitter=true</mark>\n",
    "* It is quite common that some large tables might not have the primary key or unique key and if we have to import that table using one mapper might not be feasible.\n",
    "* In that case we can specify a column by using <mark>--split-by</mark>\n",
    "    * It is a good idea to use the indexed column as part of the,<mark>--split-by</mark> otherwise, each thread might end up doing a full table scan.\n",
    "    * If there are null values in the column, corresponding records from the table will be ignored\n",
    "    * Data in the split-by column need not be unique, but if there are duplicates then there can be a skew in the data while importing (which means some files might be relatively bigger compared to other files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items_nopk \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --split-by order_item_order_id\n",
    "  \n",
    "#Splitting on text field\n",
    "sqoop import \\\n",
    "  -Dorg.apache.sqoop.splitter.allow_text_splitter=true \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table orders \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --split-by order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Reset to One Mapper\n",
    "Now let us understand how to handle those tables which do not have a primary key and entire data have to be loaded using one mapper.\n",
    "* Some tables might not have a primary key\n",
    "* If we are not sure whether the table has a primary key or not and want to use a number of mappers higher than 1, then for those tables where there is no primary key sqoop import will fail\n",
    "* One of the ways to address this issue is using.--auto-reset-to-one-mapper If there is a primary key for the table then sqoop import will use otherwise it will only use one mapper.\n",
    "* This comes handy as part of the automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items_nopk \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --autoreset-to-one-mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Formats and Compression\n",
    "Let us understand more about file formats and compression as part of Sqoop import.\n",
    "* Supported File Formats – Text file, Sequence file, Avro, Parquet etc\n",
    "    * Text file (default) <mark>--as-textfile</mark>\n",
    "    * Sequence file <mark>--as-sequencefile</mark>\n",
    "    * Avro <mark>--as-avrodatafile</mark>\n",
    "    * Parquet <mark>--as-parquetfile</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --num-mappers 2 \\\n",
    "  --as-sequencefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supported compression algorithms – snappy, gzip etc\n",
    "    * Go to **/etc/hadoop/conf** and check **core-site.xml** for supported compression codecs\n",
    "    * Use <mark>--compress</mark> to enable compression\n",
    "    * If compression codec is not specified, it will use gzip by default\n",
    "    * Compression algorithm can be specified using <mark>--compression-codec</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --num-mappers 2 \\\n",
    "  --as-textfile \\\n",
    "  --compress \\\n",
    "  --compression-codec org.apache.hadoop.io.compress.GzipCodec\n",
    "\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --num-mappers 2 \\\n",
    "  --as-textfile \\\n",
    "  --compress \\\n",
    "  --compression-codec org.apache.hadoop.io.compress.SnappyCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering of Data\n",
    "Now let us see how we can filter the data using Sqoop Import.\n",
    "* Using boundary-query\n",
    "    * Let us recap the typical import life cycle\n",
    "        * Using primary key field get min and max values (boundary query)\n",
    "        * Compute ranges using a number of mappers (splits)\n",
    "        * Establish a database connection for each split and issue a query to read the data\n",
    "        * Get data and write it o HDFS\n",
    "    * Using --boundary-query\n",
    "        * We can avoid issuing a query to get min and max values by hard-coding them (if we know the values up front)\n",
    "        * We can address the issue of outliers by narrowing down using where clause in --boundary-query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --boundary-query 'select 100000, 172198'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we want to select columns from the table while import we can use **--columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --columns order_item_order_id,order_item_id,order_item_subtotal \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \\\n",
    "  --num-mappers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can pass custom query instead of table using **--query**\n",
    "* When **--query** is used we need to specify **--split-by** or set **--num-mappers** to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --target-dir /user/dgadiraju/sqoop_import/retail_db/orders_with_revenue \\\n",
    "  --num-mappers 2 \\\n",
    "  --query \"select o.*, sum(oi.order_item_subtotal) order_revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id and \\$CONDITIONS group by o.order_id, o.order_date, o.order_customer_id, o.order_status\" \\\n",
    "  --split-by order_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimiters and Handling Nulls\n",
    "Let us see how to handle nulls and delimiters while saving data to HDFS.\n",
    "* In traditional RDBMS, nulls and delimiters are managed internally. Quite often complexity is hidden from us.\n",
    "* But when we save data in HDFS, as we have to deal with them as regular files it is our responsibility to deal with null values and delimiters.\n",
    "* By default for both strings and non-strings, Sqoop places null as placeholders for corresponding null values in the database.\n",
    "* We can provide custom values by using <mark>--null-string</mark> and <mark>--null-non-string</mark>\n",
    "* null-non-string is typically to deal with numeric fields\n",
    "* Sqoop uses comma as default delimiter when data is written to HDFS.\n",
    "* There are several control arguments to deal with delimiters\n",
    "    * <mark>--fields-terminated-by</mark> – to specify a custom field delimiter\n",
    "    * <mark>--lines-terminated-by</mark> – to specify custom line delimiter\n",
    "    * <mark>--enclosed-by</mark> – to specify an enclosing character\n",
    "    * <mark>--escaped-by</mark> – specify escape character\n",
    "    * <mark>--mysql-delimiters</mark> – to use default MySQL delimiters\n",
    "    * <mark>--optionally-enclosed-by</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Default behavior\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/hr_db \\\n",
    "  --username hr_user \\\n",
    "  --password itversity \\\n",
    "  --table employees \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/hr_db\n",
    "  \n",
    "#Changing default delimiters and nulls\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/hr_db \\\n",
    "  --username hr_user \\\n",
    "  --password itversity \\\n",
    "  --table employees \\\n",
    "  --warehouse-dir /user/dgadiraju/sqoop_import/hr_db \\\n",
    "  --null-non-string -1 \\\n",
    "  --fields-terminated-by \"\\000\" \\\n",
    "  --lines-terminated-by \":\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Loads\n",
    "We can perform incremental imports using different approaches.\n",
    "* query – we can pass a query with complete logic as we have seen before. We have to specify the split-by clause with query option to import in parallel.\n",
    "* where – we can filter data based on date field or primary key field to get an incremental load\n",
    "* Sqoop increment load arguments\n",
    "    * check-column to specify the column based on which we want to perform incremental load\n",
    "    * incremental to specify whether we want to append or last-modified (typically used to handle updates in a table)\n",
    "    * last-value – to get rows with values greater than this for the column specified in check-column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Baseline import\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --target-dir /user/dgadiraju/sqoop_import/retail_db/orders \\\n",
    "  --num-mappers 2 \\\n",
    "  --query \"select * from orders where \\$CONDITIONS and order_date like '2013-%'\" \\\n",
    "  --split-by order_id\n",
    "\n",
    "#Query can be used to load data based on condition\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --target-dir /user/dgadiraju/sqoop_import/retail_db/orders \\\n",
    "  --num-mappers 2 \\\n",
    "  --query \"select * from orders where \\$CONDITIONS and order_date like '2014-01%'\" \\\n",
    "  --split-by order_id \\\n",
    "  --append\n",
    "  \n",
    "#where in conjunction with table can be used to get data based up on a condition\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --target-dir /user/dgadiraju/sqoop_import/retail_db/orders \\\n",
    "  --num-mappers 2 \\\n",
    "  --table orders \\\n",
    "  --where \"order_date like '2014-02%'\" \\\n",
    "  --append\n",
    "\n",
    "#Incremental load using arguments specific to incremental load\n",
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --target-dir /user/dgadiraju/sqoop_import/retail_db/orders \\\n",
    "  --num-mappers 2 \\\n",
    "  --table orders \\\n",
    "  --check-column order_date \\\n",
    "  --incremental append \\\n",
    "  --last-value '2014-02-28'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
