{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Sqoop – Import into Hive Tables and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session let us understand following related to Sqoop Hive Import, import-all-tables as well as Sqoop export.\n",
    "* Create Database\n",
    "* Simple Hive import\n",
    "* Managing Hive tables\n",
    "* Import all tables\n",
    "* Data Engineering – Typical life cycle\n",
    "* Sqoop Simple Export\n",
    "* Sqoop Export – Column Mapping\n",
    "* Sqoop Export – Upsert or Merge\n",
    "* Sqoop Export – Staging Tables\n",
    "\n",
    "### Create Database\n",
    "Before getting into Hive import, it is better to create a Hive database to explore the features of Sqoop import into Hive.\n",
    "* Let us name the database as bootcampdemo\n",
    "\n",
    "### Simple Hive Import\n",
    "Let us see simple Hive import\n",
    "* <mark>--hive-import</mark> will enable hive import. It create table if it does not already exists\n",
    "* <mark>--hive-database</mark> can be used to specify the database\n",
    "* Instead of <mark>--hive-database</mark>, we can use database name as prefix as part of <mark>--hive-table</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --table order_items \\\n",
    "  --hive-import \\\n",
    "  --hive-database dgadiraju_sqoop_import \\\n",
    "  --hive-table order_items \\\n",
    "  --num-mappers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Tables\n",
    "* Default hive import behavior\n",
    "    * Create table if table does not exists\n",
    "    * If table already exists, data will be appended\n",
    "* <mark>--create-hive-table</mark> will fail hive import, if table already exists\n",
    "* <mark>--hive-overwrite</mark> will replace existing data with new set of data\n",
    "\n",
    "### Import all tables\n",
    "Sqoop provide capability to import all the tables using import-all-tables\n",
    "* All the tables from a schema/database can be imported\n",
    "* <mark>--exclude-tables</mark>, will facilitate to exclude the tables that need not be imported\n",
    "* <mark>--auto-reset-to-one-mapper</mark>, will let import all tables to choose one mapper in case table does not have primary key\n",
    "* Most of the features such as <mark>--query</mark>, <mark>--boundary-query</mark>, <mark>--where</mark> etc are not available with import-all-tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import-all-tables \\\n",
    "  --connect \"jdbc:mysql://ms.itversity.com:3306/retail_db\" \\\n",
    "  --username=retail_user \\\n",
    "  --password=itversity \\\n",
    "  --as-avrodatafile \\\n",
    "  --autoreset-to-one-mapper \\\n",
    "  --warehouse-dir=/user/itversity/sqoop_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering – Typical Life Cycle\n",
    "* Get data ingested to HDFS using Sqoop import from relational databases\n",
    "* Process data using Map Reduce or Spark\n",
    "* Processed data can be exported back to databases supporting reporting layer\n",
    "\n",
    "### Sqoop Simple Export\n",
    "As part of this topic we will run a simple export with delimiters\n",
    "* Simple export – following are the arguments we need to pass\n",
    "    * <mark>--connect</mark> with jdbc connect string. It should include target database\n",
    "    * <mark>--username</mark> and <mark>--password</mark>, the user should have right permission on the table into which data is being exported\n",
    "    * <mark>--table</mark>, target table in relational database such as MySQL into which data need to be copied\n",
    "    * <mark>--export-dir</mark> from which data need to be copied\n",
    "* Delimiters\n",
    "    * Sqoop by default expect “,” to be field delimiter\n",
    "    * But Hive default delimiter is Ascii 1 (\\001)\n",
    "    * <mark>--input-fields-terminated-by</mark> can be used to pass delimiting character other than ,\n",
    "* Number of mappers – we can increase or decrease number of threads by using <mark>--num-mappers</mark> or <mark>-m</mark>\n",
    "\n",
    "To demonstrate Sqoop Simple Export, we will perform following steps.\n",
    "* Create a table in MySQL\n",
    "* Create Hive Table by joining orders and order_items\n",
    "* Run Sqoop Export Command\n",
    "\n",
    "***Create a table in MySql***\n",
    "\n",
    "Let us create MySQL table into which data can be exported.\n",
    "* Use database retail_export if you want to create the tables and export the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```create table daily_revenue(\n",
    "  order_date varchar(30),\n",
    "  revenue float\n",
    ");```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create Hive Table***\n",
    "\n",
    "We can export data from any HDFS directory. Even when we try to export data from Hive Table we have to pass underlying directory pointed by the table.\n",
    "* Join orders and order_items\n",
    "* Compute revenue for each date\n",
    "* Use CTAS to create new table using join results\n",
    "\n",
    "***Sqoop Export Command***\n",
    "\n",
    "Let us perform simple sqoop export and understand the execution life cycle of Sqoop export\n",
    "* Read data from export directory\n",
    "* By default, Sqoop export uses 4 parallel threads to read the data by using Map Reduce split logic (based up on HDFS block size)\n",
    "* Each thread establishes database connection using JDBC url, username and password\n",
    "* Generated insert statement to load data into target table\n",
    "* Issues insert statements in the target table using connection established per thread (or mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    " --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    " --username retail_user \\\n",
    " --password itversity \\\n",
    " --export-dir /apps/hive/warehouse/dgadiraju_sqoop_import.db/daily_revenue \\\n",
    " --table daily_revenue \\\n",
    " --input-fields-terminated-by \"\\001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqoop Export – Column Mapping\n",
    "Let us see rationale behind column mapping while exporting the data\n",
    "* Some times the structure of data in HDFS and structure of table in MySQL into which data need to be exported need not match exactly\n",
    "* There is no way we can change the order of columns in our input data and we have to consume every column\n",
    "* However, Sqoop export give flexibility to map all the columns to target table columns in the order of data in HDFS. For e.g.\n",
    "    * HDFS data structure – order_date and revenue\n",
    "    * MySQL target table – revenue, order_date and description\n",
    "    * There is no description in HDFS and hence description in target table should be nullable\n",
    "    * <mark>--columns order_date,revenue</mark> will make sure data is populated into revenue and order_date in target table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```create table daily_revenue_demo (\n",
    "     revenue float,\n",
    "     order_date varchar(30),\n",
    "     description varchar(200)\n",
    ");```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    "--connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "--username retail_user \\\n",
    "--password itversity \\\n",
    "--export-dir /apps/hive/warehouse/dgadiraju_sqoop_import.db/daily_revenue \\\n",
    "--table daily_revenue_demo \\\n",
    "--columns order_date,revenue \\\n",
    "--input-fields-terminated-by \"\\001\" \\\n",
    "--num-mappers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqoop Export – Upsert/Merge\n",
    "As part of this topic we will see, how we can upsert/merge data from HDFS to MySQL tables.\n",
    "* We will first create table in hive and load data for 2013-07 daily revenue\n",
    "* Create table in MySQL\n",
    "* Run export to load 2013-07 data\n",
    "* Update mysql table with revenue to 0\n",
    "* Run export with <mark>--update-key</mark>\n",
    "* Load data into Hive table for 2013-08 data\n",
    "* Run export with <mark>--update-key</mark> and <mark>--update-mode</mark> as <mark>allowinsert</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```create table daily_revenue (\n",
    " order_date string,\n",
    " revenue float\n",
    ");```\n",
    "\n",
    "```insert into table daily_revenue\n",
    " select order_date, sum(order_item_subtotal) daily_revenue\n",
    " from orders join order_items on\n",
    " order_id = order_item_order_id\n",
    " where order_date like '2013-07%'\n",
    " group by order_date;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```create table daily_revenue (\n",
    " order_date varchar(30) primary key,\n",
    " revenue float\n",
    ");```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --export-dir /apps/hive/warehouse/bootcampdemo.db/daily_revenue \\\n",
    "  --table daily_revenue \\\n",
    "  --input-fields-terminated-by \"\\001\" \\\n",
    "  --num-mappers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```update daily_revenue set revenue = 0;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --export-dir /apps/hive/warehouse/bootcampdemo.db/daily_revenue \\\n",
    "  --table daily_revenue \\\n",
    "  --update-key order_date \\\n",
    "  --input-fields-terminated-by \"\\001\" \\\n",
    "  --num-mappers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```insert into table daily_revenue\n",
    " select order_date, sum(order_item_subtotal) daily_revenue\n",
    " from orders join order_items on\n",
    " order_id = order_item_order_id\n",
    " where order_date like '2013-08%'\n",
    " group by order_date;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    "  --connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "  --username retail_user \\\n",
    "  --password itversity \\\n",
    "  --export-dir /apps/hive/warehouse/bootcampdemo.db/daily_revenue \\\n",
    "  --table daily_revenue \\\n",
    "  --update-key order_date \\\n",
    "  --update-mode allowinsert \\\n",
    "  --input-fields-terminated-by \"\\001\" \\\n",
    "  --num-mappers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqoop Export – Stage tables\n",
    "Let us understand the relevance of stage tables as part of sqoop export.\n",
    "* Data will be read from HDFS and insert or update statements are generated to load data into MySQL tables\n",
    "* If there are any issues with data which violate constraints defined in table, Sqoop will retry 4 times before it give up.\n",
    "* Due to retries, the target table can be inconsistent state and quite often it can be tedious to clean up the target table.\n",
    "* To address this issue instead of directly loading into target table we can use stage table\n",
    "    * Data will be first loaded into stage table by sqoop export\n",
    "    * After stage table is populated with out any issues, data from stage table will be loaded into final table by issuing merge or upsert statement.\n",
    "    * We can clean up staging table by using <mark>--clear-staging-table</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```insert into table daily_revenue\n",
    " select order_date, sum(order_item_subtotal) daily_revenue\n",
    " from orders join order_items on\n",
    " order_id = order_item_order_id\n",
    " where order_date > '2013-08'\n",
    " group by order_date;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop export \\\n",
    "--connect jdbc:mysql://ms.itversity.com:3306/retail_export \\\n",
    "--username retail_user \\\n",
    "--password itversity \\\n",
    "--export-dir /apps/hive/warehouse/bootcampdemo.db/daily_revenue \\\n",
    "--table daily_revenue \\\n",
    "--staging-table daily_revenue_stage \\\n",
    "--input-fields-terminated-by \"\\001\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
