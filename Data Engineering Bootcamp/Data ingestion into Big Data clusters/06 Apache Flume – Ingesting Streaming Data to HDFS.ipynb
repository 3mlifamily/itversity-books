{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Flume – Ingesting Streaming Data to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will understand how we can use Apache Flume to ingest streaming real time data in detail.\n",
    "* Overview of Flume\n",
    "* Develop first Flume Agent\n",
    "* Understand Source, Sink and Channel\n",
    "* Flume Multi Agent Flows\n",
    "* Get data into HDFS using Flume\n",
    "* Limitations and Conclusion\n",
    "\n",
    "For this demo we will be using our [Big Data developer labs](https://labs.itversity.com/). You need to have access to existing big data cluster or sign up to our labs.\n",
    "\n",
    "### Overview of Flume\n",
    "Let us understand what Flume is all about.\n",
    "* Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.\n",
    "* It has a simple and flexible architecture based on streaming data flows.\n",
    "* It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms.\n",
    "* It can be integrated with other technologies like Spark Streaming to build streaming analytic applications.\n",
    "* We need to configure agent to get data from source like web server logs and target like data stores such as HDFS.\n",
    "* Flume agent contains – Source, Sink and Channel\n",
    "* Here is the [link](https://flume.apache.org/FlumeUserGuide.html) for latest flume documentation.\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/09/01FlumeAgentComponents.png)\n",
    "\n",
    "### Develop first Flume Agent\n",
    "Now let us go ahead and develop our first Flume agent.\n",
    "* [Here](https://flume.apache.org/FlumeUserGuide.html#a-simple-example) is the complete configuration file for simple flume example.\n",
    "* Agent Name: a1\n",
    "* Source\n",
    "    * Name: r1\n",
    "    * Type: Netcat\n",
    "    * Netcat is simple web server which runs on simple IP address and port number.\n",
    "    * IP address is either gw02.itversity.com or gw03.itversity.com on which you want to run netcat service\n",
    "    * Port Number: 44444\n",
    "    * A web server will be started by flume agent automatically\n",
    "    * This is primarily used to understand flume. We do not use this in actual implementations.\n",
    "* Sink\n",
    "    * Name: k1\n",
    "    * Type: logger\n",
    "    * Data will be sent back to the flume agent\n",
    "* Channel\n",
    "    * Name: c1\n",
    "    * Type: memory\n",
    "* Now we can start flume agent using flume-ng command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### #Create a file by name example.conf\n",
    "a1.sources = r1\n",
    "a1.sinks = k1\n",
    "a1.channels = c1\n",
    "\n",
    "a1.sources.r1.type = netcat\n",
    "a1.sources.r1.bind = 0.0.0.0\n",
    "a1.sources.r1.port = 44444\n",
    "\n",
    "a1.sinks.k1.type = logger\n",
    "\n",
    "a1.channels.c1.type = memory\n",
    "a1.channels.c1.capacity = 1000\n",
    "a1.channels.c1.transactionCapacity = 100\n",
    "\n",
    "a1.sources.r1.channels = c1\n",
    "a1.sinks.k1.channel = c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flume-ng agent \\\n",
    "  --conf-file example.conf \\\n",
    "  --name a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Source, Sink and Channel\n",
    "Each Flume agent have Source, Sink and Channel.\n",
    "* Source is primarily to read data from web server logs. There are several types of sources.\n",
    "    * netcat\n",
    "    * exec\n",
    "    * syslog\n",
    "    * avro\n",
    "    * and more.\n",
    "* Sink is primarily to write data into data stores or other Flume agent sources (via avro). There are several types of sinks.\n",
    "    * logger\n",
    "    * HDFS\n",
    "    * avro\n",
    "    * and many more\n",
    "* Channel is to channelize data between source and sink. There are different types of channels but most popular ones are memory, file and Kafka.\n",
    "* Memory gives you good performance, but not reliable. File gives you reliability at the cost of performance.\n",
    "* There can be one to many relationship between source and sink. But for each sink there need to be one channel.\n",
    "* Properties are determined by the type chosen.\n",
    "* It is easily extensible. For example we can integrate custom sources, sinks or channels with Flume.\n",
    "\n",
    "### Flume Multi Agent Flows\n",
    "Flume can be configured with multiple agents for scalability, consolidation etc.\n",
    "* Consolidation\n",
    "    * Typically to read data related to same application which is deployed on multiple web/app servers\n",
    "    * We need to define one agent for each web/app server and each of those need to write to single avro sink\n",
    "    * Then we can have one agent to read data by configuring avro source and then to what ever target you want to write to.\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/09/02FlumeConsolidation.png)\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/09/03FlumeConsolidation.png)\n",
    "\n",
    "* Multiplexing\n",
    "    * We can have data from one source multiplexed into multiple sinks.\n",
    "    * This approach is primarily used to\n",
    "        * Scaling up write operations of data\n",
    "        * Write into variety of targets such as HDFS, JMS, avro etc\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/09/04FlumeMultiplexing.png)\n",
    "\n",
    "### Get data into HDFS using Flume\n",
    "Now let us understand how to get data into HDFS using Flume.\n",
    "* Source – exec\n",
    "* Sink – HDFS\n",
    "* Channel Type – Memory\n",
    "* We will explore all the options from official documentation and then use it as part of agent definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### #flume-logger-hdfs.conf: Read data from logs and write it to both logger and hdfs\n",
    "##### #flume command to start the agent - flume-ng agent --name a1 --conf /home/dgadiraju/flume_example/example.conf --conf-file example.conf\n",
    "\n",
    "##### #Name the components on this agent\n",
    "a1.sources = logsource\n",
    "a1.sinks = loggersink hdfssink\n",
    "a1.channels = loggerchannel hdfschannel\n",
    "\n",
    "##### #Describe/configure the source\n",
    "a1.sources.logsource.type = exec\n",
    "a1.sources.logsource.command = tail -F /opt/gen_logs/logs/access.log\n",
    "\n",
    "##### #Describe the sink\n",
    "a1.sinks.loggersink.type = logger\n",
    "\n",
    "##### #Use a channel which buffers events in memory\n",
    "a1.channels.loggerchannel.type = memory\n",
    "a1.channels.loggerchannel.capacity = 1000\n",
    "a1.channels.loggerchannel.transactionCapacity = 100\n",
    "\n",
    "##### #Bind the source and sink to the channel\n",
    "a1.sources.logsource.channels = loggerchannel hdfschannel\n",
    "a1.sinks.loggersink.channel = loggerchannel\n",
    "\n",
    "##### #Describe the sink\n",
    "a1.sinks.hdfssink.type = hdfs\n",
    "a1.sinks.hdfssink.hdfs.path = hdfs://nn01.itversity.com:8020/user/dgadiraju/flume_example_%Y-%m-%d\n",
    "a1.sinks.hdfssink.hdfs.fileType = DataStream\n",
    "a1.sinks.hdfssink.hdfs.rollInterval = 120\n",
    "a1.sinks.hdfssink.hdfs.rollSize = 10485760\n",
    "a1.sinks.hdfssink.hdfs.rollCount = 30\n",
    "a1.sinks.hdfssink.hdfs.filePrefix = retail\n",
    "a1.sinks.hdfssink.hdfs.fileSuffix = .txt\n",
    "a1.sinks.hdfssink.hdfs.inUseSuffix = .tmp\n",
    "a1.sinks.hdfssink.hdfs.useLocalTimeStamp = true\n",
    "\n",
    "##### #Use a channel which buffers events in file for HDFS sink\n",
    "a1.channels.hdfschannel.type = file\n",
    "a1.channels.hdfschannel.capacity = 1000\n",
    "a1.channels.hdfschannel.transactionCapacity = 100\n",
    "a1.channels.hdfschannel.checkpointInterval = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and Conclusion\n",
    "Flume is primarily data ingestion tool.\n",
    "* It is flexible to apply rules while ingesting data.\n",
    "* We can integrate with tools like Spark Streaming, Flink, Storm etc for streaming analytics.\n",
    "* Flume is also used for getting data from existing applications web server logs into much more flexible streaming ingestion tools like Kafka. However it might be replaced with Kafka connect in future.\n",
    "* Over a period of time there can be many flume agents and there is no tools to manage them (unlike Kafka)\n",
    "* There are too many moving parts and there are simpler tools than Flume like Kafka."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
