{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges with RDBMS and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will look into some of the challenges with respect to RDBMS and how other modern databases solve the problems.\n",
    "\n",
    "* Challenges with RDBMS\n",
    "* Solutions\n",
    "* NoSql Databases\n",
    "* MongoDB\n",
    "\n",
    "### Challenges with RDBMS\n",
    "* When we perform insert or update or delete, SQL engine will perform constraint, datatypes, length and precision checks\n",
    "* Overhead for maintaining transactions (like undo, redo in case of oracle) will impact performance for heavy weight batch processing.\n",
    "* Suppose if you apply Insert statement,  it has to understand the schema and apply the schema to the record also. This causes a problem in dealing with large volume of data.\n",
    "* So, applying rules and enforcing transactions are the major bottleneck\n",
    "* Another challenge is – Modern databases have multiple servers. So, each of the nodes must get same technical view of data.\n",
    "* Joins and aggregations also become really slow, having secondary indexes becomes too expensive.\n",
    "* For example, you build e-commerce platform and want to find out the performance of each of the department, performance of platform with in geographic solution etc. To build these reports we need to process huge amounts of data from multiple database or perform expensive joins with in each database, this will take huge amount of resources – CPU, memory and networking and thus makes applications run slow.\n",
    "\n",
    "### Solutions\n",
    "* If you need to find out about your business trends, you need to get reports. Getting reports from a given database is resource intensive.\n",
    "* Solution is Data Warehousing. Data Warehouses are central repositories of integrated data coming from various sources to get useful business insights.\n",
    "* Examples of Data Warehousing technologies – Teradata, Hadoop etc.\n",
    "* Process of getting data into datawarehouse-\n",
    "    * Identify all sources from which data needs to be fetched.\n",
    "    * Data Modeling using Dimension Modeling\n",
    "    * Develop and Schedule ETL (Extract, Transform, Load) jobs\n",
    "* Data is fetched from traditional based systems into Data Warehouse, thus it can store historical data and also we need not write expensive queries on source data.\n",
    "* This process of getting data into datawarehouse is called ETL.\n",
    "\n",
    "### Data Modeling in Data Warehouses\n",
    "* In traditional database, we model our data using normalization and application starts loading data into the tables and business happens as usual .\n",
    "* But when it comes to Data Warehouse, even though now the report requirements are running in Data Warehouse still meeting that SLA of running the report (it can be daily or monthly or yearly). The executive management wants to see these reports as soon as possible. If we create reports on normalized tables, it can be slow due to expensive joins.\n",
    "* So, we use Dimensional Modeling  to create tables – facts, dimensions and measures.\n",
    "* Denormalized Data Model- star schema and snowflake schema.\n",
    "* If I want to compute daily report, if data is precomputed for each day, performance will be faster.\n",
    "* Redshift, Vertica and Hive datawarehouse tools don’t enforce constraints.\n",
    "* **Example of creating a fact table for daily revenue for each product**-We have two tables,orders and order_items.Now let us create a denormalized data model(include only those fields which we require for generating our report) – "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create table order_revenue_fact (\n",
    "  order_date date,\n",
    "  product_name varchar(200),\n",
    "  order_item_subtotal\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created lowest granular fact table which will result in large table over a period of time. We can have raw data into this table and build another table where we pre-aggregate on daily basis for each product. **order_item_subtotal** can be used as a **measure** to evaluate daily revenue or daily revenue per product and order_date and product_name are the keys. This new table which contain pre-aggregated data will have as many products are sold on each day.\n",
    "\n",
    "* Dimension – Dimension is just metadata which drive your report requirement. If we want see monthly report,we extract from order_date to get monthly reports.Having date dimension,we can find out how my business is performing on particular day.Dimension is just metadata which drive your report requirement.Similarly Product dimension has categoryand product name.Using category you can get the product name.Without the dimensions, we cannot measure the facts.\n",
    "* Fact table is the one which contain keys and measures depending up on the report requirements.\n",
    "\n",
    "### NOSQL Databases\n",
    "* Examples – Cassandra, Hbase and MongoDB\n",
    "* Tables are generally Indexed and partitioned\n",
    "* NoSQL not suitable for mission critical solutions like e-commerce platforms.\n",
    "* Works well for scalability of simple tables\n",
    "* Have flexible schema\n",
    "* Let us understand NoSQL databases with an example of LinkedIn skills endorsement. If we define the data model using RDBMS, we need to define 3 tables -Skills (id, name)  endorsement (id, who, whom) and person(id, name, company name, photo) and for every query we need to join these three tables which is very expensive and non-scalable in case of millions of records.\n",
    "* In such cases, it’s highly preferable to use NOSQL databases.We can create two tables – one with raw data and other with pre-processed data as per our reporting requirements. In raw table we have each endorsement and pre aggregated data in another table.  Pre aggregating can be achieved using Kafka and other streaming technologies such as Spark Streaming.\n",
    "\n",
    "### Working with MongoDB\n",
    "* Log in to mongodb shell – mongo –host gw01.itversity.com\n",
    "* You can have multiple databases in one mongo installation.\n",
    "* List databases – show dbs;\n",
    "* Collection term is used in place of tables and document for rows.\n",
    "* Collection is a group of documents\n",
    "* List collections – show collections;\n",
    "* CRUD operations\n",
    "    * Inserting records – db.demo.insert({“emp_id”:1,”emp_name”:”scott tiger”})\n",
    "    * Now show collections command will also include demo in the list.\n",
    "    * db.demo.find({}) is equivalent to select all records from demo.\n",
    "    * Use db.demo.find({}).pretty()  command to get the data in formatted manner\n",
    "    * db.demo.findOne({}) will fetch first record\n",
    "    * db.demo.find({“emp_id”:2}) will fetch emp_id 2 record.\n",
    "    * Create database using command – –use retail_db_demo\n",
    "    * Using mongo import\n",
    "        * mongoimport –db retail_db –collection departments –type json –columnsHaveTypes –file /example/file.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
