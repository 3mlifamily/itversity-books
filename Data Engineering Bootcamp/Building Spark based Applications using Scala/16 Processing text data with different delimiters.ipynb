{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text data with different delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic and next few topics let us see different file formats supported Spark along with compression algorithms. File formats include csv, orc, parquet, avro etc and compression algorithms include gzip, deflate, snappy etc.\n",
    "\n",
    "* Dealing with different delimiters for text file format (e. g: yelp data set)\n",
    "* Different file formats and APIs associated with them\n",
    "* Spark 2.x have support to these file formats out of the box\n",
    "    * json\n",
    "    * csv\n",
    "    * orc\n",
    "    * parquet\n",
    "    * and more\n",
    "* We can also use 3rd party APIs to read data from file formats such as Avro\n",
    "* Compression can be extensively used to save storage requirements and also to improve performance\n",
    "* For each file format, compressing data is a bit different\n",
    "* Most of the file formats such as orc, parquet etc compress data by default\n",
    "\n",
    "### Custom Record Delimiter\n",
    "* sc.textFile or spark.read.csv works fine as long as record delimiter is new line character\n",
    "* But if record delimiter is any other character than new line, then we have to use lower level HDFS APIs such as org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n",
    "* Spark Context (sc) have API called newAPIHadoopFile to use lower level HDFS APIs\n",
    "* newAPIHadoopFile takes 5 arguments\n",
    "    * path\n",
    "    * input file format\n",
    "    * key type\n",
    "    * value type\n",
    "    * configuration\n",
    "* We need to first get hadoop configuration from spark context and set textinputformat.record.delimiter\n",
    "* Key type and value type are purely based on the file format. For text file format, key type is org.apache.hadoop.io.LongWritable and value type is org.apache.hadoop.io.Text\n",
    "* To preview the data we have to convert into toString as part of map\n",
    "* You can see the complete code snippet here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:35: error: not found: value path\n",
       "       val yelpReview = sc.newAPIHadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
       "                                            ^\n",
       "<console>:42: error: not found: value al\n",
       "val $ires7 = al.path\n",
       "             ^\n",
       "<console>:27: error: not found: value al\n",
       "       al path = \"/public/yelp-dataset/yelp_review.csv\"\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al path = \"/public/yelp-dataset/yelp_review.csv\"\n",
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"textinputformat.record.delimiter\", \"\\r\")\n",
    "\n",
    "import org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n",
    "import org.apache.hadoop.io.LongWritable\n",
    "import org.apache.hadoop.io.Text\n",
    "\n",
    "val yelpReview = sc.newAPIHadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf)\n",
    "yelpReview.count\n",
    "yelpReview.map(r => r._2.toString).take(10).foreach(println)\n",
    "yelpReview.map(r => (r._2.toString.split(\"\\\",\\\"\").size, 1)).reduceByKey(_ + _).collect.foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
