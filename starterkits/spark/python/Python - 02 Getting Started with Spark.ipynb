{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Getting Started with Spark\n",
    "\n",
    "As part of this module we will take a simple use case and try to scratch the surface of the Spark. We will be using simple use case to demontrate end to end Data Engineering Pipeline.\n",
    "\n",
    "* Understand Data Model\n",
    "* Define Problem Statement\n",
    "* Creating Spark Context\n",
    "* Setting Run Time Job Properties\n",
    "* Reading data from CSV Files\n",
    "* Apply Filtering\n",
    "* Row Level Transformations\n",
    "* Perform Joins\n",
    "* Aggregate Data\n",
    "* Perform Sorting\n",
    "* Write output to Files\n",
    "* Complete Script\n",
    "* Validating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data Model\n",
    "\n",
    "Let us understand the data model and also characteristics of the data.\n",
    "\n",
    "* Base directory for **retail_db** data sets is **/public/retail_db**.\n",
    "* It have six folders, each folder represents a separate table.\n",
    "  * Product Catalog Tables\n",
    "    * products\n",
    "    * categories\n",
    "    * departments\n",
    "  * Customers Table\n",
    "    * customers\n",
    "  * Transactional Tables\n",
    "    * orders\n",
    "    * order_items\n",
    "* **orders** and **order_items** are related. **orders** is parent table and **order_items** is child table for orders.\n",
    "* All folders have one ore more files under them.\n",
    "* Each line represents a record and have values related to multiple columns. Each record is delimited or separated by **comma (,)**.\n",
    "* First field in each orders record is order_id and it is a primary key (unique and not null)\n",
    "* Second field in each order_items record is order_item_order_id which is a foreign key attribute to orders order_id.\n",
    "* There are other relationships as well, however they are not relevant to get started. We will primarily focus on orders and order_items data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem Statement\n",
    "\n",
    "Get monthly revenue considering complete or closed orders\n",
    "\n",
    "* We will use orders and order_items data.\n",
    "* **orders** is available at **/public/retail_db/orders**\n",
    "* **order_items** is available at **/public/retail_db/order_items**\n",
    "* We need to consider orders with COMPLETE or CLOSED status.\n",
    "* Revenue can be computed using **order_item_subtotal** which is 5th attribute in order_items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Context\n",
    "\n",
    "Let us understand how to create Spark Context using `SparkSession` from `pyspark.sql`.\n",
    "\n",
    "* We need to have spark context to leverage both APIs as well as distributed computing framework.\n",
    "* `SparkSession` is a wrapper class which will use existing Spark Context or create new one.\n",
    "* We can customize the behavior of Spark Context created by passing properties using `config` or by using APIs such as `appName`, `master` etc.\n",
    "* APIs are provided only for most commonly used properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Run Time Job Properties\n",
    "\n",
    "Let us understand how to customize run time behavior of submitted jobs.\n",
    "\n",
    "* Once Spark Context is created, we can customize run time behavior by using `spark.conf.set`. \n",
    "* In our case let us set a property called as `spark.sql.shuffle.partitions` to 2.\n",
    "* If we do not set this property, by default it will use 200 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from CSV Files\n",
    "\n",
    "Let us quickly see how we can read data from CSV Files.\n",
    "\n",
    "* Spark provide several APIs to read the files of different file formats.\n",
    "* All the out of the box APIs are available under `spark.read`.\n",
    "* In our case we have to read text files where each record is delimited or separated by comma (',').\n",
    "* To create Data Frames for `orders` and `order_items` we can pass the path to `spark.read.csv`.\n",
    "* There are other options as well which can be passed using keyword arguments. You can run help on `spark.read.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Filtering\n",
    "\n",
    "Let us see how we can filter out records in Data Frame.\n",
    "* We can either use `filter` or `where` to filter the data. Both of them serve the same purpose.\n",
    "* We can pass the condictions either in SQL Style or API Style.\n",
    "* In this case, we have used SQL Style to check `order_status` for `COMPLETE` or `CLOSED` orders.\n",
    "* We can perform all standard filtering conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example for API Style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    filter(orders.order_status.isin('COMPLETE', 'CLOSED')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Level Transformations\n",
    "\n",
    "Let us see how we can project and also derive new fields out of existing fields leveraging functions.\n",
    "\n",
    "* One of the ways to project data is by using `select` on top of Data Frame.\n",
    "* Spark provides almost 300 pre defined functions as part of `pyspark.sql.functions`.\n",
    "* In our case we need to import and use `date_format` function to extract month from existing date.\n",
    "* Later we will also import and use functions such as `sum` and `round` while aggregating the data.\n",
    "* We can also provide meaningful names to derived fields using `alias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "orders_transformed = orders_filtered. \\\n",
    "    select('order_id', date_format('order_date', 'yyyyMM').alias('order_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Joins\n",
    "\n",
    "Let us join both the data sets which have the fields we are interested in. \n",
    "\n",
    "* We can join data sets using `join`.\n",
    "* We also might have to pass join condition in case the column names are different between the data sets.\n",
    "* In our case we have to join `orders` and `order_items` using `orders.order_id` and `order_items.order_item_order_id`.\n",
    "\n",
    "We can join original Data Frames as well and generate order_month while grouping the data as demonstrated in the **Complete Script** Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month = orders_transformed. \\\n",
    "    join(order_items, \n",
    "         orders.order_id == order_items.order_item_order_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Data\n",
    "\n",
    "As we have joined `orders` and `order_items`, let us perform the aggregation.\n",
    "* In this case want to compute revenue for each month.\n",
    "* `order_month` is derived field which contain both year and month.\n",
    "* We can use `order_month` as part of `groupBy` so that data can be grouped. It will generate a special Data Frame of type `GroupedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month. \\\n",
    "    groupBy('order_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now invoke aggregate functions such as `sum` and pass the desired field using which we want to aggregate (in this case we can pass `order_item_subtotal` to `sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "monthly_revenue = order_details_by_month. \\\n",
    "    groupBy('order_month'). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sorting\n",
    "\n",
    "As we got the revenue for each month, let us sort the data so that we can review the output for the validation.\n",
    "\n",
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* By default data will be sorted in ascending order.\n",
    "* In this case we are sorting the data by `order_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted = monthly_revenue. \\\n",
    "    orderBy('order_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output to Files\n",
    "\n",
    "As data is read, processed and sorted - now it is time to write data to files in underlying file system.\n",
    "\n",
    "* In our environment **/public** is read only folder. You will not be able to add files under subdirectories of **/public**.\n",
    "* Assuming you have write access to **/user/[OS_USER_NAME]**, I have used **/user/{username}/retail_db/monthly_revenue** as target folder.\n",
    "* `{username}` is replaced by the OS user used for login using `getpass.getuser()`.\n",
    "* `coalesce(1)` is used to write the output to one file.\n",
    "* If the folder and files already exists, `mode('overwrite')` will replace existing folder with new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "monthly_revenue_sorted. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Script\n",
    "\n",
    "Here is the complete script or program which takes care of the following:\n",
    "\n",
    "* Create Spark Context and set the properties.\n",
    "* Read the data related to different tables.\n",
    "* Process the data using relevant Spark Data Frame APIs.\n",
    "* Write the data back to file system\n",
    "\n",
    "Entire Data processing and writing the data back to file system is developed using Piped approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, sum, round\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )\n",
    "\n",
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )\n",
    "\n",
    "orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "    join(order_items, orders.order_id == order_items.order_item_order_id). \\\n",
    "    groupBy(date_format('order_date', 'yyyyMM').alias('order_month')). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue')). \\\n",
    "    orderBy('order_month'). \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Output\n",
    "\n",
    "Let us go ahead and validate the output.\n",
    "\n",
    "* In our case we are using HDFS and hence we should be able to use HDFS commands to validate.\n",
    "* Let us first list the files which will give some idea about when they are created.\n",
    "* For some file formats, we will also see extension as well as compression algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of small text files we can use `cat` to see the contents. It might not work if the files are compressed.\n",
    "* Also, it is not a good practice to use `cat` for larger text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -cat /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
